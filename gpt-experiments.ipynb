{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U transformers datasets peft tf-keras sacrebleu rouge_score pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from datasets import Dataset\n",
    "# Load the E2E NLG Challenge dataset\n",
    "dataset_e2e = load_dataset('e2e_nlg')\n",
    "\n",
    "# Dictionary to store datasets\n",
    "datasets = {\n",
    "    'E2E': dataset_e2e\n",
    "}\n",
    "\n",
    "# Hyperparameters for each dataset\n",
    "hyperparams = {\n",
    "    'E2E': {\n",
    "        'weight_decay': 0.01,\n",
    "        'dropout_prob': 0.1,\n",
    "        'label_smooth': 0.1,\n",
    "        'length_penalty': 0.9\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping function for E2E NLG test dataset\n",
    "def group_e2e_test_data(test_data):\n",
    "    df = pd.DataFrame(test_data)\n",
    "    df.sort_values(by='meaning_representation', inplace=True)\n",
    "    grouped = df.groupby('meaning_representation')['human_reference'].apply(list).reset_index()\n",
    "    grouped_dataset = Dataset.from_pandas(grouped)\n",
    "    return grouped_dataset\n",
    "\n",
    "def preprocess_e2e(examples):\n",
    "    inputs = examples['meaning_representation']\n",
    "    targets = examples['human_reference']\n",
    "    texts = [inp + ' ' + tgt for inp, tgt in zip(inputs, targets)]\n",
    "    model_inputs = tokenizer(texts, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_e2e_eval(examples):\n",
    "    inputs = examples['meaning_representation']\n",
    "    targets = examples['human_reference']\n",
    "    model_inputs = tokenizer(inputs, truncation=True)\n",
    "    model_inputs[\"meaning_representation\"] = inputs\n",
    "    model_inputs[\"human_reference\"] = targets\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    human_references = [item['human_reference'] for item in batch]\n",
    "    meaning_representations = [item['meaning_representation'] for item in batch]\n",
    "    # Remove 'human_reference' before using data_collator\n",
    "    batch = [{'input_ids': item['input_ids'], 'attention_mask': item['attention_mask']} for item in batch]\n",
    "    batch = data_collator(batch)\n",
    "    batch['human_reference'] = human_references\n",
    "    batch['meaning_representation'] = meaning_representations\n",
    "    return batch\n",
    "\n",
    "def generate_predictions(test_dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    progress_bar = tqdm(test_dataloader, desc=\"Generating predictions\")\n",
    "    for batch in progress_bar:\n",
    "        # Use only the meaning representation as input\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        meaning_representations = batch['meaning_representation']\n",
    "        human_references = batch['human_reference']\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_beams=10,\n",
    "                length_penalty=0.9,\n",
    "                no_repeat_ngram_size=4,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        for i, output in enumerate(output_ids):\n",
    "            prediction = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            input_text = meaning_representations[i]\n",
    "            if prediction.startswith(input_text):\n",
    "                prediction = prediction[len(input_text):].strip()\n",
    "            predictions.append(prediction)\n",
    "            references.append(human_references[i])\n",
    "    return predictions, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the training and evaluation loop\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import sacrebleu\n",
    "import torch\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    print(f\"Training on {dataset_name} dataset\")\n",
    "    params = hyperparams[dataset_name]\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'left'\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Apply LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"c_attn\"],\n",
    "        lora_dropout=params['dropout_prob'],\n",
    "        init_lora_weights=\"gaussian\",\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Preprocess the dataset using the appropriate function\n",
    "    if dataset_name == 'E2E':\n",
    "        test_data = group_e2e_test_data(dataset['test'])\n",
    "        preprocess_function = preprocess_e2e\n",
    "        preprocess_function_eval = preprocess_e2e_eval\n",
    "    \n",
    "    train_data = dataset['train']\n",
    "    \n",
    "    train_tokenized = train_data.map(preprocess_function, batched=True, remove_columns=train_data.column_names)\n",
    "    test_tokenized = test_data.map(preprocess_function_eval, batched=True, remove_columns=train_data.column_names)\n",
    "    \n",
    "    # Data collator and DataLoaders\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, return_tensors=\"pt\", mlm=False)\n",
    "    train_dataloader = DataLoader(train_tokenized, shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "    test_dataloader = DataLoader(test_tokenized, batch_size=8, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=params['weight_decay'])\n",
    "    num_epochs = 5\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=num_training_steps)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [{dataset_name}]\")\n",
    "        for batch in progress_bar:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    # Evaluation\n",
    "    predictions, references = generate_predictions(test_dataloader, model, tokenizer, params['length_penalty'])\n",
    "    bleu = sacrebleu.corpus_bleu(predictions, references)\n",
    "    print(f\"{dataset_name} BLEU score: {bleu.score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
