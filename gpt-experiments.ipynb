{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U transformers datasets peft evaluate tf-keras sacrebleu rouge_score pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from datasets import Dataset\n",
    "# Load the E2E NLG Challenge dataset\n",
    "dataset = load_dataset('e2e_nlg')\n",
    "\n",
    "train_data = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "\n",
    "test_data = dataset['test']\n",
    "test_data = sorted(test_data, key=lambda x: x['meaning_representation'])\n",
    "\n",
    "test_data = [list(group) for key, group in\n",
    "                groupby(test_data, key=lambda x: x['meaning_representation'])]\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"meaning_representation\": group[0]['meaning_representation'],\n",
    "        \"human_reference\": [item['human_reference'] for item in group]\n",
    "    }\n",
    "    for group in test_data\n",
    "]\n",
    "\n",
    "test_data = Dataset.from_pandas(pd.DataFrame(test_data))\n",
    "\n",
    "# Preprocess the dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Add padding token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [mr for mr in examples['meaning_representation']]\n",
    "    targets = [text for text in examples['human_reference']]\n",
    "    texts = [\"Input: \" + inp + ' Output: ' + tgt + \" \" + tokenizer.eos_token for inp, tgt in zip(inputs, targets)]\n",
    "    model_inputs = tokenizer(texts, truncation=True, max_length=512)\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function_eval(examples):\n",
    "    inputs = [mr for mr in examples['meaning_representation']]\n",
    "    inputs = [\"Input: \" + inp + \" Output: \" for inp in inputs]\n",
    "    model_inputs = tokenizer(inputs, truncation=True, max_length=512)\n",
    "    model_inputs[\"meaning_representation\"] = inputs\n",
    "    model_inputs[\"human_reference\"] = examples[\"human_reference\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_tokenized = train_data.map(preprocess_function, batched=True)\n",
    "validation_tokenized = validation_data.map(preprocess_function, batched=True)\n",
    "test_tokenized = test_data.map(preprocess_function_eval, batched=True)\n",
    "\n",
    "train_tokenized = train_tokenized.remove_columns(train_data.column_names)\n",
    "validation_tokenized = validation_tokenized.remove_columns(validation_data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 Medium model and apply LoRA to Wq and Wv\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "\n",
    "# Prepare LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\"],  # Applies to Wq and Wv\n",
    "    lora_dropout=0.1,\n",
    "    init_lora_weights=\"gaussian\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nlg-metricverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load('bleu')\n",
    "meteor = evaluate.load('meteor')\n",
    "rouge = evaluate.load('rouge')\n",
    "nist = evaluate.load('nist_mt')\n",
    "#cider = evaluate.load(\"Kamichanw/CIDEr\")\n",
    "from nlgmetricverse import NLGMetricverse, load_metric\n",
    "cider = NLGMetricverse(metrics=load_metric(\"cider\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    human_references = [item['human_reference'] for item in batch]\n",
    "    meaning_representations = [item['meaning_representation'] for item in batch]\n",
    "    # Remove 'human_reference' before using data_collator\n",
    "    batch = [{'input_ids': item['input_ids'], 'attention_mask': item['attention_mask']} for item in batch]\n",
    "    batch = data_collator(batch)\n",
    "    batch['human_reference'] = human_references\n",
    "    batch['meaning_representation'] = meaning_representations\n",
    "    return batch\n",
    "\n",
    "def generate_predictions(test_dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    progress_bar = tqdm(test_dataloader, desc=\"Generating predictions\")\n",
    "    for batch in progress_bar:\n",
    "        # Use only the meaning representation as input\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        meaning_representations = batch['meaning_representation']\n",
    "        human_references = batch['human_reference']\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_beams=10,\n",
    "                length_penalty=0.9,\n",
    "                no_repeat_ngram_size=4,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        for i, output in enumerate(output_ids):\n",
    "            prediction = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            input_text = meaning_representations[i]\n",
    "            if prediction.startswith(input_text):\n",
    "                prediction = prediction[len(input_text):].strip()\n",
    "            predictions.append(prediction)\n",
    "            references.append(human_references[i])\n",
    "    return predictions, references\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_tokenized,\n",
    "    batch_size=8,\n",
    "    collate_fn=custom_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_dataloader = DataLoader(train_tokenized, shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "validation_dataloader = DataLoader(validation_tokenized, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=num_training_steps)\n",
    "\n",
    "# Track best validation loss\n",
    "best_eval_loss = float('inf')\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in progress_bar:\n",
    "        inputs = batch.to(model.device)\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    # Evaluation\n",
    "    predictions, references = generate_predictions(test_dataloader)\n",
    "\n",
    "    # Compute metrics\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "    meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "    nist_score = nist.compute(predictions=predictions, references=references)\n",
    "    #cider_score = cider.compute(predictions, references)\n",
    "    \n",
    "    cider_score = cider(predictions=predictions, references=references)\n",
    "\n",
    "    test_metrics = {\n",
    "        'bleu': bleu_score['bleu'],\n",
    "        'meteor': meteor_score['meteor'],\n",
    "        'rouge_l': rouge_score['rougeL'],\n",
    "        'nist': nist_score['nist_mt'],\n",
    "        'cider': cider_score['cider']['score'],\n",
    "    }\n",
    "    print(test_metrics)\n",
    "    \"\"\"for batch in validation_dataloader:\n",
    "        with torch.no_grad():\n",
    "            inputs = batch.to(model.device)\n",
    "            outputs = model(**inputs)\n",
    "            eval_loss += outputs.loss.item()\n",
    "    avg_eval_loss = eval_loss / len(validation_dataloader)\n",
    "    print(f\"Validation Loss: {avg_eval_loss}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if avg_eval_loss < best_eval_loss:\n",
    "        best_eval_loss = avg_eval_loss\n",
    "        torch.save(model.state_dict(), \"./results/best_model.bin\")\"\"\"\n",
    "\n",
    "#model.load_state_dict(torch.load(\"./results/best_model.bin\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
