{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U transformers datasets peft evaluate tf-keras sacrebleu rouge_score pycocoevalcap nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from datasets import Dataset\n",
    "# Load the E2E NLG Challenge dataset\n",
    "dataset_e2e = load_dataset('e2e_nlg')\n",
    "dataset_webnlg = load_dataset('web_nlg', 'webnlg_challenge_2017')\n",
    "dataset_dart = load_dataset('dart')\n",
    "\n",
    "# Dictionary to store datasets\n",
    "datasets = {\n",
    "    'E2E': dataset_e2e,\n",
    "}\n",
    "\n",
    "# Hyperparameters for each dataset\n",
    "hyperparams = {\n",
    "    'E2E': {\n",
    "        'weight_decay': 0.01,\n",
    "        'dropout_prob': 0.1,\n",
    "        'label_smooth': 0.1,\n",
    "        'length_penalty': 0.9\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping function for E2E NLG test dataset\n",
    "def group_e2e_test_data(test_data):\n",
    "    df = pd.DataFrame(test_data)\n",
    "    df.sort_values(by='meaning_representation', inplace=True)\n",
    "    grouped = df.groupby('meaning_representation')['human_reference'].apply(list).reset_index()\n",
    "    grouped_dataset = Dataset.from_pandas(grouped)\n",
    "    return grouped_dataset\n",
    "\n",
    "def preprocess_e2e(examples):\n",
    "    inputs = examples['meaning_representation']\n",
    "    targets = examples['human_reference']\n",
    "    texts = [inp + ' | ' + tgt + \" \" + tokenizer.eos_token for inp, tgt in zip(inputs, targets)]\n",
    "    model_inputs = tokenizer(texts, truncation=True)\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_e2e_eval(examples):\n",
    "    inputs = examples['meaning_representation']\n",
    "    targets = examples['human_reference']\n",
    "    texts = [inp + ' | ' for inp in inputs]\n",
    "    model_inputs = tokenizer(texts, truncation=True)\n",
    "    model_inputs[\"meaning_representation\"] = texts\n",
    "    model_inputs[\"human_reference\"] = targets\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install nlg-metricverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load('bleu')\n",
    "rouge = evaluate.load('rouge')\n",
    "nist = evaluate.load('nist_mt')\n",
    "from nlgmetricverse import NLGMetricverse, load_metric\n",
    "cider = NLGMetricverse(metrics=load_metric(\"cider\"))\n",
    "meteor = NLGMetricverse(metrics=load_metric(\"meteor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    human_references = [item['human_reference'] for item in batch]\n",
    "    meaning_representations = [item['meaning_representation'] for item in batch]\n",
    "    # Remove 'human_reference' before using data_collator\n",
    "    batch = [{'input_ids': item['input_ids'], 'attention_mask': item['attention_mask']} for item in batch]\n",
    "    batch = data_collator(batch)\n",
    "    batch['human_reference'] = human_references\n",
    "    batch['meaning_representation'] = meaning_representations\n",
    "    return batch\n",
    "\n",
    "def generate_predictions(test_dataloader, model, tokenizer, length_penalty):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    progress_bar = tqdm(test_dataloader, desc=\"Generating predictions\")\n",
    "    for batch in progress_bar:\n",
    "        # Use only the meaning representation as input\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        meaning_representations = batch['meaning_representation']\n",
    "        human_references = batch['human_reference']\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_beams=10,\n",
    "                length_penalty=length_penalty,\n",
    "                repetition_penalty=1.0,\n",
    "                no_repeat_ngram_size=4,\n",
    "                max_new_tokens=64,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        for i, output in enumerate(output_ids):\n",
    "            prediction = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            input_text = meaning_representations[i]\n",
    "            if prediction.startswith(input_text):\n",
    "                prediction = prediction[len(input_text) :]\n",
    "            prediction = prediction.strip()\n",
    "            #predictions.append(prediction)\n",
    "            #references.append(human_references[i])\n",
    "            predictions.append(format_outputs(prediction))\n",
    "            references.append([format_outputs(human_reference) for human_reference in human_references[i]]) \n",
    "    return predictions, references\n",
    "\n",
    "def format_outputs(text):\n",
    "    text = text.lower()\n",
    "    text = ' '.join(re.split('(\\W)', text))\n",
    "    text = text.split()\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the training and evaluation loop\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import re\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    print(f\"Training on {dataset_name} dataset\")\n",
    "    params = hyperparams[dataset_name]\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "    #tokenizer.pad_token = tokenizer.eos_token\n",
    "    #tokenizer.pad_token_id = 18610\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    " \n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "    with torch.no_grad():\n",
    "      model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    # Apply LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"c_attn\"],\n",
    "        lora_dropout=params['dropout_prob'],\n",
    "        init_lora_weights=\"gaussian\",\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    # Preprocess the dataset using the appropriate function\n",
    "    if dataset_name == 'E2E':\n",
    "        test_data = group_e2e_test_data(dataset['test'])\n",
    "        preprocess_function = preprocess_e2e\n",
    "        preprocess_function_eval = preprocess_e2e_eval\n",
    "    \n",
    "    train_data = dataset['train']\n",
    "    \n",
    "    train_tokenized = train_data.map(preprocess_function, batched=True, remove_columns=train_data.column_names)\n",
    "    test_tokenized = test_data.map(preprocess_function_eval, batched=True, remove_columns=test_data.column_names)\n",
    "    \n",
    "    # Data collator and DataLoaders\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    train_dataloader = DataLoader(train_tokenized, shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "    test_dataloader = DataLoader(test_tokenized, batch_size=8, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=params['weight_decay'])\n",
    "    num_epochs = 5\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=num_training_steps)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [{dataset_name}]\")\n",
    "        for batch in progress_bar:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "        # Evaluation\n",
    "        predictions, references = generate_predictions(test_dataloader, model, tokenizer, params['length_penalty'])\n",
    "        bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "        meteor_score = meteor(predictions=predictions, references=references)\n",
    "        rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "        nist_score = nist.compute(predictions=predictions, references=references)\n",
    "        cider_score = cider(predictions=predictions, references=references)\n",
    "        \n",
    "        test_metrics = {\n",
    "            'bleu': bleu_score['bleu'],\n",
    "            'meteor': meteor_score['meteor']['score'],\n",
    "            'rouge_l': rouge_score['rougeL'],\n",
    "            'nist': nist_score['nist_mt'],\n",
    "            'cider': cider_score['cider']['score']\n",
    "        }\n",
    "        \n",
    "        print(test_metrics)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
