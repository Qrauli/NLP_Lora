{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/yikaiyang/.local/lib/python3.13/site-packages (4.47.1)\n",
      "Requirement already satisfied: datasets in /home/yikaiyang/.local/lib/python3.13/site-packages (3.2.0)\n",
      "Requirement already satisfied: accelerate in /home/yikaiyang/.local/lib/python3.13/site-packages (1.2.1)\n",
      "Requirement already satisfied: peft in /home/yikaiyang/.local/lib/python3.13/site-packages (0.14.0)\n",
      "Requirement already satisfied: evaluate in /home/yikaiyang/.local/lib/python3.13/site-packages (0.4.3)\n",
      "Requirement already satisfied: torch in /home/yikaiyang/.local/lib/python3.13/site-packages (2.5.1)\n",
      "Requirement already satisfied: ipywidgets in /home/yikaiyang/.local/lib/python3.13/site-packages (8.1.5)\n",
      "Requirement already satisfied: filelock in /home/yikaiyang/.local/lib/python3.13/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/yikaiyang/.local/lib/python3.13/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/yikaiyang/.local/lib/python3.13/site-packages (from transformers) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yikaiyang/.local/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/yikaiyang/.asdf/installs/python/3.13.1/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/yikaiyang/.asdf/installs/python/3.13.1/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/yikaiyang/.asdf/installs/python/3.13.1/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/yikaiyang/.local/lib/python3.13/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/yikaiyang/.local/lib/python3.13/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/yikaiyang/.local/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/yikaiyang/.local/lib/python3.13/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/yikaiyang/.local/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/yikaiyang/.local/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/yikaiyang/.local/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/yikaiyang/.local/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/yikaiyang/.local/lib/python3.13/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/yikaiyang/.local/lib/python3.13/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: psutil in /home/yikaiyang/.local/lib/python3.13/site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/yikaiyang/.asdf/installs/python/3.13.1/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/yikaiyang/.local/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/yikaiyang/.local/lib/python3.13/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/yikaiyang/.local/lib/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/yikaiyang/.local/lib/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/yikaiyang/.local/lib/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/yikaiyang/.local/lib/python3.13/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/yikaiyang/.local/lib/python3.13/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/yikaiyang/.local/lib/python3.13/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/yikaiyang/.local/lib/python3.13/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/yikaiyang/.local/lib/python3.13/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/yikaiyang/.local/lib/python3.13/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/yikaiyang/.local/lib/python3.13/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/yikaiyang/.local/lib/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/yikaiyang/.local/lib/python3.13/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: setuptools in /home/yikaiyang/.asdf/installs/python/3.13.1/lib/python3.13/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/yikaiyang/.local/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/yikaiyang/.local/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/yikaiyang/.local/lib/python3.13/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/yikaiyang/.local/lib/python3.13/site-packages (from ipywidgets) (8.31.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/yikaiyang/.local/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /home/yikaiyang/.local/lib/python3.13/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /home/yikaiyang/.local/lib/python3.13/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/yikaiyang/.local/lib/python3.13/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yikaiyang/.local/lib/python3.13/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yikaiyang/.local/lib/python3.13/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yikaiyang/.local/lib/python3.13/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yikaiyang/.local/lib/python3.13/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/yikaiyang/.local/lib/python3.13/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/yikaiyang/.local/lib/python3.13/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: decorator in /home/yikaiyang/.local/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/yikaiyang/.local/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/yikaiyang/.local/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/yikaiyang/.local/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/yikaiyang/.local/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/yikaiyang/.local/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack_data in /home/yikaiyang/.local/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yikaiyang/.asdf/installs/python/3.13.1/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yikaiyang/.asdf/installs/python/3.13.1/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yikaiyang/.asdf/installs/python/3.13.1/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yikaiyang/.asdf/installs/python/3.13.1/lib/python3.13/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yikaiyang/.asdf/installs/python/3.13.1/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yikaiyang/.local/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yikaiyang/.local/lib/python3.13/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yikaiyang/.local/lib/python3.13/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/yikaiyang/.local/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/yikaiyang/.local/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/yikaiyang/.local/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /home/yikaiyang/.local/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/yikaiyang/.local/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/yikaiyang/.local/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/yikaiyang/.local/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets accelerate peft evaluate torch ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "825b8d70fe314b80a629f0e4585ccfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from peft import LoraModel, LoraConfig\n",
    "from evaluate import load\n",
    "import math\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2. 1B General model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|start_header_id|>',\n",
       " 'user',\n",
       " '<|end_header_id|>',\n",
       " 'What',\n",
       " 'Ġis',\n",
       " 'Ġa',\n",
       " 'Ġdog',\n",
       " '?',\n",
       " '<|eot_id|>',\n",
       " '<|start_header_id|>',\n",
       " 'assistant',\n",
       " '<|end_header_id|>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = \"<|start_header_id|>user<|end_header_id|>What is a dog?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "#input_str = \"The future of AI is\"\n",
    "inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "tokenizer.tokenize(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = tokenizer(\"Hello, how are you\", return_tensors=\"pt\").input_ids.to(device)\n",
    "attention_mask = inputs['attention_mask']\n",
    "input_ids = inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,    882, 128007,   3923,    374,    264,   5679,     30,\n",
       "         128009, 128006,  78191, 128007]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default tokenizer already inserts <|begin_of_text|> (128000) token by itself. No need to add it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128001"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "output_ids = model.generate(inputs['input_ids'], pad_token_id=tokenizer.eos_token_id, max_length=20, do_sample=True, top_p=0.95, top_k=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000,    791,   3938,    315,  15592,    374,   1618,    198,   2170,\n",
       "           279,   1917,  21234,    311,   1977,    279,   1828,   9659,    315,\n",
       "         21075,  11478])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>What is a dog?<|eot_id|><|start_header_id|>assistant<|end_header_id|>useredReader?＼\n",
      "＼\n",
      "The dog\n"
     ]
    }
   ],
   "source": [
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLama 3.2 1B Instruct/Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "intruct_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "instruct_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|start_header_id|>', 'user', '<|end_header_id|>', 'What', 'Ġis', 'Ġa', 'Ġdog', '?', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "input_str = \"<|start_header_id|>user<|end_header_id|>What is a dog?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "#input_str = \"### Human: What is a dog? ### Assistant:\"\n",
    "inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "print(tokenizer.tokenize(input_str))\n",
    "attention_mask = inputs['attention_mask']\n",
    "input_ids = inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>What is a dog?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nA dog is a domesticated carnivorous mammal that belongs to the family Canidae. Dogs are known for their loyalty, intelligence, and versatility, making them one of the most popular pets and working animals in the world.\\n\\nHere are some key characteristics of a dog:\\n\\n**Physical Characteristics:**\\n\\n* Dogs have a muscular body,'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_text_instruct(input_ids, max_length = 80, device='cpu'):\n",
    "    instruct_model.to(device)\n",
    "    output_ids = instruct_model.generate(input_ids, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=True, top_p=0.95, top_k=60)\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "    return output_text\n",
    "\n",
    "generate_text_instruct(inputs['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000, 128006,    882, 128007,   3923,    374,    264,   5679,     30,\n",
       "        128009, 128006,  78191, 128007,    271,     32,   5679,    374,    264,\n",
       "         13018,    660,  36041,    278,    430,  17623,    311,    279,   3070,\n",
       "          3053, 114405,     13,  39525,    527,  15499,   5552,    311,  56271,\n",
       "           323,   4430,   1690,   7106,    323,  36695,  17910,    449,   1124,\n",
       "            13,   5810,    527,   1063,   1401,  13363,    922,  12875,   1473,\n",
       "           334,  40353,  85084,     25,  57277,      9,  39525,    527,  11383,\n",
       "          1990,    220,    605,     12,   1187,  15271,    320,    914,     12,\n",
       "          5547,  10166,      8,  16615,    323,  17988,   1990,    220])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with built in apply chat template method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 29 Dec 2024\\n\\nYou are an helpful assistantuser\\n\\nhelloassistant\\n\\nyeah'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "conv = [\n",
    "{\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are an helpful assistant\"\n",
    "},    \n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"hello\",\n",
    "}, {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"yeah\"\n",
    "}\n",
    "]\n",
    "\n",
    "#conv = \"### Human: What is a dog? ### Assistant: Hello\"\n",
    "conv_tok = tokenizer.apply_chat_template(conv)\n",
    "tokenizer.decode(conv_tok, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 29 Dec 2024\\n\\nYou are an helpful assistantuser\\n\\nhelloassistant\\n\\nyeahsystemreadystatechange\\n\\nyou're the best!\\n\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ## Try it on general llama 3.2 model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "conv = [\n",
    "{\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are an helpful assistant\"\n",
    "},    \n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"hello\",\n",
    "}, {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"yeah\"\n",
    "}\n",
    "]\n",
    "\n",
    "#conv = \"### Human: What is a dog? ### Assistant: Hello\"\n",
    "inputs = tokenizer.apply_chat_template(conv, tokenize=True, return_tensors='pt')\n",
    "#tokenizer.decode(conv_tok)\n",
    "model.to(\"cpu\")\n",
    "output_ids = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, max_length=80, do_sample=True, top_p=0.95, top_k=60)\n",
    "tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
