{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from evaluate import load\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load and preprocess E2E NLG dataset\n",
    "def load_and_preprocess_e2e():\n",
    "    data = load_dataset(\"e2e_nlg\")\n",
    "\n",
    "    def preprocess_e2e(example):\n",
    "        input_text = \"MR: \" + example['meaning_representation']\n",
    "        target_text = \"Output: \" + example['human_reference']\n",
    "        return {\n",
    "            \"input_text\": input_text,\n",
    "            \"target_text\": target_text\n",
    "        }\n",
    "\n",
    "    processed_data = data.map(preprocess_e2e)\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(\n",
    "            example['input_text'],\n",
    "            text_target=example['target_text'],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128\n",
    "        )\n",
    "\n",
    "    # Tokenize datasets\n",
    "    tokenized_train = processed_data['train'].map(tokenize_function, batched=True)\n",
    "    tokenized_val = processed_data['validation'].map(tokenize_function, batched=True)\n",
    "\n",
    "    return tokenized_train, tokenized_val\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load datasets\n",
    "tokenized_train, tokenized_val = load_and_preprocess_e2e()\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ2SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Load evaluation metrics\n",
    "bleu = load(\"bleu\")\n",
    "nist = load(\"nist_mt\")\n",
    "meteor = load(\"meteor\")\n",
    "rouge = load(\"rouge\")\n",
    "cider = load(\"cider\")\n",
    "\n",
    "# Compute metrics\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(logits, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    bleu_score = bleu.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n",
    "    nist_score = nist.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n",
    "    meteor_score = meteor.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n",
    "    rouge_score = rouge.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n",
    "    cider_score = cider.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_score[\"bleu\"],\n",
    "        \"nist\": nist_score[\"nist_mt\"],\n",
    "        \"meteor\": meteor_score[\"meteor\"],\n",
    "        \"rouge_l\": rouge_score[\"rougeL\"].mid.fmeasure,\n",
    "        \"cider\": cider_score[\"CIDEr\"]\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_e2e\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_e2e\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Validation Results: {eval_results}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"lora_gpt2_e2e\")\n",
    "print(\"Model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
