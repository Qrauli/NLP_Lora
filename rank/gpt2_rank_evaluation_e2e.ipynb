{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6df3b22-738a-4742-8810-35250363f965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.45.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting peft\n",
      "  Using cached peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting tf-keras\n",
      "  Using cached tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting sacrebleu\n",
      "  Using cached sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting rouge_score\n",
      "  Using cached rouge_score-0.1.2-py3-none-any.whl\n",
      "Collecting pycocoevalcap\n",
      "  Using cached pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from peft) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.11/site-packages (from peft) (2.1.2+cu121)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Using cached accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting tensorflow<2.19,>=2.18 (from tf-keras)\n",
      "  Using cached tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Using cached portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.11/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.11/site-packages (from sacrebleu) (5.3.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.11/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from rouge_score) (1.16.0)\n",
      "Collecting pycocotools>=2.0.2 (from pycocoevalcap)\n",
      "  Using cached pycocotools-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.25.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.69.0)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Using cached transformers-4.48.1-py3-none-any.whl (9.7 MB)\n",
      "Using cached datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Using cached peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Using cached tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Using cached pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
      "Using cached accelerate-1.3.0-py3-none-any.whl (336 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached pycocotools-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (458 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Installing collected packages: xxhash, tabulate, portalocker, dill, tensorboard, sacrebleu, rouge_score, multiprocess, tokenizers, pycocotools, accelerate, transformers, tensorflow, pycocoevalcap, datasets, tf-keras, peft, evaluate\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.17.1\n",
      "    Uninstalling tensorboard-2.17.1:\n",
      "      Successfully uninstalled tensorboard-2.17.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.3\n",
      "    Uninstalling tokenizers-0.20.3:\n",
      "      Successfully uninstalled tokenizers-0.20.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.45.1\n",
      "    Uninstalling transformers-4.45.1:\n",
      "      Successfully uninstalled transformers-4.45.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.1\n",
      "    Uninstalling tensorflow-2.17.1:\n",
      "      Successfully uninstalled tensorflow-2.17.1\n",
      "Successfully installed accelerate-1.3.0 datasets-3.2.0 dill-0.3.8 evaluate-0.4.3 multiprocess-0.70.16 peft-0.14.0 portalocker-3.1.1 pycocoevalcap-1.2 pycocotools-2.0.8 rouge_score-0.1.2 sacrebleu-2.5.1 tabulate-0.9.0 tensorboard-2.18.0 tensorflow-2.18.0 tf-keras-2.18.0 tokenizers-0.21.0 transformers-4.48.1 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U transformers datasets peft evaluate tf-keras sacrebleu rouge_score pycocoevalcap nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6007529e-3527-4e7c-94b1-1c034b1cca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-26 19:26:57.564414: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-26 19:26:57.596936: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737919617.618988     285 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737919617.625455     285 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-26 19:26:57.657761: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from datasets import Dataset\n",
    "# Update the training and evaluation loop\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83a8ade9-4be5-456e-9630-9a3b54222e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a131b40-242f-4c87-8b9d-09dac917329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "alpha=32\n",
    "weight_decay = 0.01\n",
    "dropout_prob = 0.1\n",
    "label_smooth = 0.1\n",
    "length_penalty = 0.9\n",
    "max_length = 128\n",
    "batch_size=8\n",
    "lr=2e-4\n",
    "num_epochs=5\n",
    "num_warmup_steps=500\n",
    "train_set_size=None\n",
    "eval_set_size=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4674c7e-3a37-455f-b294-b63613ee8723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 1212, 318, 281, 1672, 6827, 329, 262, 4279, 12660, 4876, 13]], 'attention_mask': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]This is an example sentence for the rank evaluation task.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/preprocessing\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "model_name = \"gpt2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) #GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "encoded_input = tokenizer([\"This is an example sentence for the rank evaluation task.\"], truncation=True, padding='max_length', max_length=128)\n",
    "print(encoded_input)\n",
    "print(tokenizer.decode(encoded_input[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06050d40-6194-4e1c-be81-41ecef78fad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meaning_representation': 'name[The Vaults], eatType[pub], priceRange[more than £30], customer rating[5 out of 5], near[Café Adriatic]',\n",
       " 'human_reference': 'The Vaults pub near Café Adriatic has a 5 star rating.  Prices start at £30.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from datasets import disable_caching\n",
    "#disable_caching()\n",
    "dataset = load_dataset(\"e2e_nlg\")\n",
    "\n",
    "train_set = dataset[\"train\"]\n",
    "val_set = dataset[\"validation\"]\n",
    "test_set = dataset[\"test\"]\n",
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bae81c40-c7db-47ac-a222-20588af57983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c0fa95cb9f47a199d704b6dd3ab3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42061 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739790ea5eef47d0b128d8093149ef2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafafecbf4454ed19c3f8be48b5b406b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"meaning_representation\"], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05cae5d3-c76f-4030-a2b8-83eb160a3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping function for E2E NLG test dataset\n",
    "def group_e2e_test_data(test_data):\n",
    "    df = pd.DataFrame(test_data)\n",
    "    df.sort_values(by='meaning_representation', inplace=True)\n",
    "    grouped = df.groupby('meaning_representation')['human_reference'].apply(list).reset_index()\n",
    "    grouped_dataset = Dataset.from_pandas(grouped)\n",
    "    return grouped_dataset\n",
    "\n",
    "def preprocess_e2e(examples):\n",
    "    inputs = examples['meaning_representation']\n",
    "    targets = examples['human_reference']\n",
    "    texts = [inp + ' | ' + tgt + \" \" + tokenizer.eos_token for inp, tgt in zip(inputs, targets)]\n",
    "    model_inputs = tokenizer(texts, truncation=True)\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_e2e_eval(examples):\n",
    "    inputs = examples['meaning_representation']\n",
    "    targets = examples['human_reference']\n",
    "    texts = [inp + ' | ' for inp in inputs]\n",
    "    model_inputs = tokenizer(texts, truncation=True)\n",
    "    model_inputs[\"meaning_representation\"] = texts\n",
    "    model_inputs[\"human_reference\"] = targets\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cd6ecfc-6bef-4562-a5d5-6697a26b3216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlg-metricverse\n",
      "  Using cached nlg_metricverse-0.9.9-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting datasets<2.10,>=2.9 (from nlg-metricverse)\n",
      "  Using cached datasets-2.9.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting fire>=0.4.0 (from nlg-metricverse)\n",
      "  Using cached fire-0.7.0-py3-none-any.whl\n",
      "Collecting nltk<3.7.1,>=3.6.6 (from nlg-metricverse)\n",
      "  Using cached nltk-3.7-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from nlg-metricverse) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /opt/conda/lib/python3.11/site-packages (from nlg-metricverse) (2.2.3)\n",
      "Requirement already satisfied: rouge-score==0.1.2 in /opt/conda/lib/python3.11/site-packages (from nlg-metricverse) (0.1.2)\n",
      "Requirement already satisfied: setuptools>=65.5.1 in /opt/conda/lib/python3.11/site-packages (from nlg-metricverse) (75.1.0)\n",
      "Requirement already satisfied: requests>=2.27.1 in /opt/conda/lib/python3.11/site-packages (from nlg-metricverse) (2.32.3)\n",
      "Collecting click==8.1.3 (from nlg-metricverse)\n",
      "  Using cached click-8.1.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting syllables>=1.0.3 (from nlg-metricverse)\n",
      "  Using cached syllables-1.0.9-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting typing>=3.7.4.3 (from nlg-metricverse)\n",
      "  Using cached typing-3.7.4.3-py3-none-any.whl\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.11/site-packages (from nlg-metricverse) (24.1)\n",
      "Requirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.11/site-packages (from nlg-metricverse) (1.13.1)\n",
      "Requirement already satisfied: matplotlib>=3.5.1 in /opt/conda/lib/python3.11/site-packages (from nlg-metricverse) (3.9.2)\n",
      "Collecting textstat>=0.7.3 (from nlg-metricverse)\n",
      "  Using cached textstat-0.7.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting codecarbon==2.1.4 (from nlg-metricverse)\n",
      "  Using cached codecarbon-2.1.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting validators>=0.20.0 (from nlg-metricverse)\n",
      "  Using cached validators-0.34.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in /opt/conda/lib/python3.11/site-packages (from nlg-metricverse) (0.13.2)\n",
      "Requirement already satisfied: torch>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from nlg-metricverse) (2.1.2+cu121)\n",
      "Requirement already satisfied: transformers>=4.24.0 in /opt/conda/lib/python3.11/site-packages (from nlg-metricverse) (4.48.1)\n",
      "Collecting bert-score>=0.3.11 (from nlg-metricverse)\n",
      "  Using cached bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.11/site-packages (from nlg-metricverse) (4.66.5)\n",
      "Requirement already satisfied: evaluate>=0.4 in /opt/conda/lib/python3.11/site-packages (from nlg-metricverse) (0.4.3)\n",
      "Collecting pyemd>=0.5.1 (from nlg-metricverse)\n",
      "  Using cached pyemd-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: ipython>=7.16.1 in /opt/conda/lib/python3.11/site-packages (from nlg-metricverse) (8.28.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from nlg-metricverse) (1.5.2)\n",
      "Requirement already satisfied: arrow in /opt/conda/lib/python3.11/site-packages (from codecarbon==2.1.4->nlg-metricverse) (1.3.0)\n",
      "Collecting pynvml (from codecarbon==2.1.4->nlg-metricverse)\n",
      "  Using cached pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from codecarbon==2.1.4->nlg-metricverse) (5.9.8)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.11/site-packages (from codecarbon==2.1.4->nlg-metricverse) (9.0.0)\n",
      "Collecting fuzzywuzzy (from codecarbon==2.1.4->nlg-metricverse)\n",
      "  Using cached fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.11/site-packages (from rouge-score==0.1.2->nlg-metricverse) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from rouge-score==0.1.2->nlg-metricverse) (1.16.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets<2.10,>=2.9->nlg-metricverse) (17.0.0)\n",
      "Collecting dill<0.3.7 (from datasets<2.10,>=2.9->nlg-metricverse)\n",
      "  Using cached dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets<2.10,>=2.9->nlg-metricverse) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets<2.10,>=2.9->nlg-metricverse) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]>=2021.11.1->datasets<2.10,>=2.9->nlg-metricverse) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets<2.10,>=2.9->nlg-metricverse) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from datasets<2.10,>=2.9->nlg-metricverse) (0.27.1)\n",
      "Collecting responses<0.19 (from datasets<2.10,>=2.9->nlg-metricverse)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets<2.10,>=2.9->nlg-metricverse) (6.0.2)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.11/site-packages (from fire>=0.4.0->nlg-metricverse) (2.5.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython>=7.16.1->nlg-metricverse) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython>=7.16.1->nlg-metricverse) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython>=7.16.1->nlg-metricverse) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.11/site-packages (from ipython>=7.16.1->nlg-metricverse) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython>=7.16.1->nlg-metricverse) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython>=7.16.1->nlg-metricverse) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/conda/lib/python3.11/site-packages (from ipython>=7.16.1->nlg-metricverse) (5.14.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /opt/conda/lib/python3.11/site-packages (from ipython>=7.16.1->nlg-metricverse) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython>=7.16.1->nlg-metricverse) (4.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.1->nlg-metricverse) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.1->nlg-metricverse) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.1->nlg-metricverse) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.1->nlg-metricverse) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.1->nlg-metricverse) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.1->nlg-metricverse) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.1->nlg-metricverse) (2.9.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk<3.7.1,>=3.6.6->nlg-metricverse) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk<3.7.1,>=3.6.6->nlg-metricverse) (2024.11.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.1.5->nlg-metricverse) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.1.5->nlg-metricverse) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.27.1->nlg-metricverse) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.27.1->nlg-metricverse) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.27.1->nlg-metricverse) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.27.1->nlg-metricverse) (2024.8.30)\n",
      "Collecting cmudict<2.0.0,>=1.0.11 (from syllables>=1.0.3->nlg-metricverse)\n",
      "  Using cached cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting importlib-metadata<7.0,>=5.1 (from syllables>=1.0.3->nlg-metricverse)\n",
      "  Using cached importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting pyphen (from textstat>=0.7.3->nlg-metricverse)\n",
      "  Using cached pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.12.0->nlg-metricverse) (3.16.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.12.0->nlg-metricverse) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.12.0->nlg-metricverse) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.12.0->nlg-metricverse) (3.1.4)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.12.0->nlg-metricverse) (2.1.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.24.0->nlg-metricverse) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.24.0->nlg-metricverse) (0.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->nlg-metricverse) (3.5.0)\n",
      "Requirement already satisfied: importlib-resources>=5 in /opt/conda/lib/python3.11/site-packages (from cmudict<2.0.0,>=1.0.11->syllables>=1.0.3->nlg-metricverse) (6.4.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets<2.10,>=2.9->nlg-metricverse) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets<2.10,>=2.9->nlg-metricverse) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets<2.10,>=2.9->nlg-metricverse) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets<2.10,>=2.9->nlg-metricverse) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets<2.10,>=2.9->nlg-metricverse) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets<2.10,>=2.9->nlg-metricverse) (1.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata<7.0,>=5.1->syllables>=1.0.3->nlg-metricverse) (3.20.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython>=7.16.1->nlg-metricverse) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython>=7.16.1->nlg-metricverse) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.16.1->nlg-metricverse) (0.2.13)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.11/site-packages (from arrow->codecarbon==2.1.4->nlg-metricverse) (2.9.0.20241003)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.12.0->nlg-metricverse) (3.0.1)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets<2.10,>=2.9->nlg-metricverse)\n",
      "  Using cached multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml->codecarbon==2.1.4->nlg-metricverse)\n",
      "  Using cached nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=7.16.1->nlg-metricverse) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=7.16.1->nlg-metricverse) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=7.16.1->nlg-metricverse) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.12.0->nlg-metricverse) (1.3.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets<2.10,>=2.9->nlg-metricverse) (0.2.0)\n",
      "Using cached nlg_metricverse-0.9.9-py3-none-any.whl (201 kB)\n",
      "Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Using cached codecarbon-2.1.4-py3-none-any.whl (174 kB)\n",
      "Using cached bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Using cached datasets-2.9.0-py3-none-any.whl (462 kB)\n",
      "Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Using cached pyemd-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (666 kB)\n",
      "Using cached syllables-1.0.9-py3-none-any.whl (15 kB)\n",
      "Using cached textstat-0.7.4-py3-none-any.whl (105 kB)\n",
      "Using cached validators-0.34.0-py3-none-any.whl (43 kB)\n",
      "Using cached cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
      "Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Using cached importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Using cached fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Using cached multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "Using cached pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Using cached pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
      "Using cached nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: nvidia-ml-py, fuzzywuzzy, validators, typing, pyphen, pynvml, pyemd, importlib-metadata, fire, dill, click, textstat, responses, nltk, multiprocess, cmudict, syllables, codecarbon, datasets, bert-score, nlg-metricverse\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib_metadata 8.5.0\n",
      "    Uninstalling importlib_metadata-8.5.0:\n",
      "      Successfully uninstalled importlib_metadata-8.5.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.8\n",
      "    Uninstalling dill-0.3.8:\n",
      "      Successfully uninstalled dill-0.3.8\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.7\n",
      "    Uninstalling click-8.1.7:\n",
      "      Successfully uninstalled click-8.1.7\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.9.1\n",
      "    Uninstalling nltk-3.9.1:\n",
      "      Successfully uninstalled nltk-3.9.1\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.16\n",
      "    Uninstalling multiprocess-0.70.16:\n",
      "      Successfully uninstalled multiprocess-0.70.16\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.2.0\n",
      "    Uninstalling datasets-3.2.0:\n",
      "      Successfully uninstalled datasets-3.2.0\n",
      "Successfully installed bert-score-0.3.13 click-8.1.3 cmudict-1.0.32 codecarbon-2.1.4 datasets-2.9.0 dill-0.3.6 fire-0.7.0 fuzzywuzzy-0.18.0 importlib-metadata-6.11.0 multiprocess-0.70.14 nlg-metricverse-0.9.9 nltk-3.7 nvidia-ml-py-12.570.86 pyemd-1.0.0 pynvml-12.0.0 pyphen-0.17.2 responses-0.18.0 syllables-1.0.9 textstat-0.7.4 typing-3.7.4.3 validators-0.34.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# needs to be installed after dataset has been loaded\n",
    "%pip install nlg-metricverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1c6e04e-5b04-45ac-9166-9be8391e20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlgmetricverse import NLGMetricverse, load_metric\n",
    "\n",
    "bleu = evaluate.load('bleu')\n",
    "rouge = evaluate.load('rouge')\n",
    "nist = evaluate.load('nist_mt')\n",
    "cider = NLGMetricverse(metrics=load_metric(\"cider\"))\n",
    "meteor = NLGMetricverse(metrics=load_metric(\"meteor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc726236-33bd-4bae-a5bf-da73f04fc5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    human_references = [item['human_reference'] for item in batch]\n",
    "    meaning_representations = [item['meaning_representation'] for item in batch]\n",
    "    # Remove 'human_reference' before using data_collator\n",
    "    batch = [{'input_ids': item['input_ids'], 'attention_mask': item['attention_mask']} for item in batch]\n",
    "    batch = data_collator(batch)\n",
    "    batch['human_reference'] = human_references\n",
    "    batch['meaning_representation'] = meaning_representations\n",
    "    return batch\n",
    "\n",
    "def generate_predictions(test_dataloader, model, tokenizer, length_penalty):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    progress_bar = tqdm(test_dataloader, desc=\"Generating predictions\")\n",
    "    for batch in progress_bar:\n",
    "        # Use only the meaning representation as input\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        meaning_representations = batch['meaning_representation']\n",
    "        human_references = batch['human_reference']\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_beams=10,\n",
    "                length_penalty=length_penalty,\n",
    "                repetition_penalty=1.0,\n",
    "                no_repeat_ngram_size=4,\n",
    "                max_new_tokens=64,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        for i, output in enumerate(output_ids):\n",
    "            prediction = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            input_text = meaning_representations[i]\n",
    "            #if i < 5:\n",
    "            #    print(\"Prediction:\")\n",
    "            #    print(prediction[0])\n",
    "            #    print(\"References:\")\n",
    "            #    print(human_references[i])\n",
    "            prediction = prediction.strip()\n",
    "            predictions.append(format_outputs(prediction))\n",
    "            references.append([format_outputs(human_reference) for human_reference in human_references[i]]) \n",
    "    return predictions, references\n",
    "\n",
    "def format_outputs(text):\n",
    "    text = text.lower()\n",
    "    text = ' '.join(re.split('(\\W)', text))\n",
    "    text = text.split()\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d294b6d-abb9-4a56-95c1-31c839c0f81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2d6b1f3841494aba605abfc245d291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42061 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f56148bb6c04224a869276044599861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = group_e2e_test_data(dataset['test'])\n",
    "preprocess_function = preprocess_e2e\n",
    "preprocess_function_eval = preprocess_e2e_eval\n",
    "\n",
    "train_data = dataset['train']\n",
    "\n",
    "train_tokenized = train_data.map(preprocess_function, batched=True, remove_columns=train_data.column_names)\n",
    "if train_set_size is not None:\n",
    "    train_tokenized = train_tokenized.select(range(train_set_size))\n",
    "test_tokenized = test_data.map(preprocess_function_eval, batched=True, remove_columns=test_data.column_names)\n",
    "if eval_set_size is not None:\n",
    "    test_tokenized = test_tokenized.select(range(eval_set_size))\n",
    "# Data collator and DataLoaders\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_tokenized, shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(test_tokenized, batch_size=batch_size, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069dde0-46e8-4bac-879d-61ea274a6077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Rank: 1\n",
      "trainable params: 184,320 || all params: 774,215,680 || trainable%: 0.0238\n",
      "None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd63bfca9094dc4bf3a2f7c49c70619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/5258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for rank in ranks:\n",
    "    peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=rank,\n",
    "    lora_alpha=alpha,\n",
    "    target_modules=[\"c_attn\"],\n",
    "    lora_dropout=dropout_prob,\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    bias=\"none\"\n",
    "    )\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    # FREEZE WEIGHTS\n",
    "    #for param in model.parameters():\n",
    "    #    param.requires_grad = False\n",
    "    if rank != 0:\n",
    "        model = get_peft_model(model, peft_config)\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    print(\"Evaluation for Rank: \" + str(rank))\n",
    "    if rank != 0:\n",
    "        print(model.print_trainable_parameters())\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    # Evaluation\n",
    "    predictions, references = generate_predictions(test_dataloader, model, tokenizer, length_penalty=length_penalty)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "    meteor_score = meteor(predictions=predictions, references=references)\n",
    "    rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "    nist_score = nist.compute(predictions=predictions, references=references)\n",
    "    cider_score = cider(predictions=predictions, references=references)\n",
    "\n",
    "    test_metrics = {\n",
    "        'bleu': bleu_score['bleu'],\n",
    "        'meteor': meteor_score['meteor']['score'],\n",
    "        'rouge_l': rouge_score['rougeL'],\n",
    "        'nist': nist_score['nist_mt'],\n",
    "        'cider': cider_score['cider']['score']\n",
    "    }\n",
    "\n",
    "    print(test_metrics)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PyTorch Trainer Framework\n",
    "Initially, this notebook was aimed to be based on the transformer's Trainer API (https://huggingface.co/docs/transformers/main_classes/trainer). However, we faced OutOfMemoryExceptions during evaluation. Also, we were not able to properly adjust the hyperparameters as described by LoRA."
   ],
   "id": "582b6d91912222d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7dfbb2-392a-4456-b353-62bc77807044",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U transformers sentence-transformers datasets peft evaluate tf-keras sacrebleu rouge_score nltk #  fsspec==2023.9.2 pycocoevalcap git+https://github.com/salaniz/pycocoevalcap git+https://github.com/disi-unibo-nlp/nlg-metricverse#-quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3718971-5b9e-4d20-a4b0-7b0f0bcfdf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq, AutoTokenizer, AutoModelForCausalLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from evaluate import load\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece34e96-e4b5-4f2a-9e2d-534546a5d608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "511725ed-0c1a-49f6-831d-0f2319a32dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 1212, 318, 281, 1672, 6827, 329, 262, 4279, 12660, 4876, 13]], 'attention_mask': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>This is an example sentence for the rank evaluation task.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/preprocessing\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) #GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "encoded_input = tokenizer([\"This is an example sentence for the rank evaluation task.\"], truncation=True, padding='max_length', max_length=128)\n",
    "print(encoded_input)\n",
    "print(tokenizer.decode(encoded_input[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca596f39-3510-49e1-b765-3b86a38b67ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meaning_representation': 'name[The Vaults], eatType[pub], priceRange[more than £30], customer rating[5 out of 5], near[Café Adriatic]',\n",
       " 'human_reference': 'The Vaults pub near Café Adriatic has a 5 star rating.  Prices start at £30.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"e2e_nlg\")\n",
    "\n",
    "train_set = dataset[\"train\"]\n",
    "val_set = dataset[\"validation\"]\n",
    "test_set = dataset[\"test\"]\n",
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f60e80-6e51-4588-9ef7-266adef5568d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be998854972e4075a3406622dfce1c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42061 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7732a180b4a64f35a5283f0b7515d883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45457d25122426ca25a668d0864bd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"meaning_representation\"], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1765358e-6061-4e19-b136-e2718f49a15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc808ba384814a3b8efb1b1a266f4c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42061 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82054da4557e4fcf9f44a41e1f2ea57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a40fd292d524db9851f15bdecff2cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation example:\n",
      "Reference:\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>There is a place in the city centre, Alimentum, that is not family-friendly.\n",
      "Model Input:\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Meaning Representation: name[Alimentum], area[city centre], familyFriendly[no] Human Reference: \n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/evaluate/transformers_integrations\n",
    "# https://medium.com/@prashanth.ramanathan/fine-tuning-a-pre-trained-gpt-2-model-and-performing-inference-a-hands-on-guide-57c097a3b810\n",
    "#def preprocess_function(examples):\n",
    "#    model_inputs = tokenizer(examples[\"meaning_representation\"], truncation=True, padding='max_length', max_length=128)\n",
    "#    labels = tokenizer(text_target=examples[\"human_reference\"], truncation=True, padding='max_length', max_length=128)\n",
    "#\n",
    "#    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#    return model_inputs\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"meaning_representation\"], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "    examples[\"prediction\"] = \"Meaning Representation: \" + examples[\"meaning_representation\"] + \" Human Reference: \" + examples[\"human_reference\"]\n",
    "    return tokenizer(examples[\"prediction\"], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "def preprocess_function_alt(examples):\n",
    "    model_inputs = tokenizer(examples[\"meaning_representation\"], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "    examples[\"prediction\"] = \"Meaning Representation: \" + examples[\"meaning_representation\"] + \" Human Reference: \" + examples[\"human_reference\"]\n",
    "    tokenized = tokenizer(examples[\"prediction\"], truncation=True, padding='max_length', max_length=128)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "def preprocess_function_val(examples):\n",
    "    model_inputs = tokenizer(\"Meaning Representation: \" + examples[\"meaning_representation\"] + \" Human Reference: \", truncation=True, padding='max_length', max_length=128)\n",
    "    labels = tokenizer(text_target=examples[\"human_reference\"], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "preprocessed_datasets = dict()\n",
    "preprocessed_datasets[\"train\"] = dataset[\"train\"].map(preprocess_function)\n",
    "preprocessed_datasets[\"validation\"] = dataset[\"validation\"].map(preprocess_function_val)\n",
    "preprocessed_datasets[\"test\"] = dataset[\"test\"].map(preprocess_function_val)\n",
    "\n",
    "print(\"Validation example:\")\n",
    "print(\"Reference:\")\n",
    "print(tokenizer.decode(preprocessed_datasets['validation'][0][\"labels\"]))\n",
    "print(\"Model Input:\")\n",
    "print(tokenizer.decode(preprocessed_datasets['validation'][0][\"input_ids\"]))\n",
    "preprocessed_datasets[\"train\"] = preprocessed_datasets[\"train\"].remove_columns([\"prediction\", \"meaning_representation\", \"human_reference\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be3d367b-9138-4c22-a22d-2a7e93f19e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank=64\n",
    "alpha=32\n",
    "dropout=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bfdd1d61-7d1d-40df-a73e-d164db09f84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,291,456 || all params: 361,114,624 || trainable%: 1.7422\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=rank,\n",
    "    lora_alpha=alpha,\n",
    "    #target_modules=[\"c_attn\"],\n",
    "    lora_dropout=dropout,\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, max_length=128)\n",
    "# FREEZE WEIGHTS\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = False\n",
    "model = get_peft_model(model, peft_config)\n",
    "model = model.to(device)\n",
    "#with torch.no_grad():\n",
    "#    model.resize_token_embeddings(len(tokenizer))\n",
    "#model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f8e16c5-8b21-4399-851f-8b2b9baad236",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "training_args = TrainingArguments(output_dir=\"logs_gpt2_rank_e2e_00\",\n",
    "                                         #eval_strategy=\"steps\",\n",
    "                                         #learning_rate=2e-4,\n",
    "                                         per_device_train_batch_size=64,\n",
    "                                         #gradient_accumulation_steps=4,\n",
    "                                         #weight_decay=0.01,\n",
    "                                         #save_total_limit=3,\n",
    "                                         #num_train_epochs=2,\n",
    "                                         remove_unused_columns=False,\n",
    "                                         warmup_steps=500,\n",
    "                                         #weight_decay=0.01,\n",
    "                                         #fp16=True,\n",
    "                                         #predict_with_generate=True,\n",
    "                                         #eval_steps=50,\n",
    "                                         logging_dir=\"./logs_gpt2_rank_e2e_00\",\n",
    "                                         logging_steps=100,\n",
    "                                         max_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd7ff4c3-2cef-4232-9cd1-9b1cc685d88c",
   "metadata": {},
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_datasets[\"train\"],\n",
    "    #eval_dataset=preprocessed_datasets[\"validation\"].shuffle().select(range(128)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a287d580-d427-4a5a-add7-a842312058d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meaning Representation: name[Cocum], eatType[coffee shop], food[Chinese], priceRange[high], customer rating[1 out of 5], familyFriendly[yes] Human Reference:  \"I'm a coffee shop owner, and I'm a family friend.\"\n",
      "The first thing I noticed was that the name was a little different. I was expecting a coffee shop, but instead I got a coffee shop. I'm not sure why, but I'm guessing that the name was chosen because it's a coffee shop. I'm not sure if it's because I'm a coffee shop owner,\n"
     ]
    }
   ],
   "source": [
    "# Generation example\n",
    "inputs = tokenizer('Meaning Representation: name[Cocum], eatType[coffee shop], food[Chinese], priceRange[high], customer rating[1 out of 5], familyFriendly[yes] Human Reference: ', return_tensors='pt')\n",
    "outputs = model.generate(**inputs.to(device), max_length=128, num_return_sequences=1)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "metadata": {
    "scrolled": true
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue Spice is a coffee shop in city centre.\n",
      "\n",
      " Blue Spice is a coffee shop in the city centre with a price range of £20-25-25. It is located in the centre of the city centre. It has a customer rating of 3 out of 5, and it is located near the corner of the City Centre. It is called Blue Spice. It is\n"
     ]
    }
   ],
   "execution_count": 39,
   "source": [
    "print(references[1])\n",
    "print()\n",
    "print(predictions[1])"
   ],
   "id": "b10a99fb-7878-45e3-883b-93c076efc9b3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
